<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Compare robustness of nonnegative matrix factorization algorithms</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="report.tex"> 
<link rel="stylesheet" type="text/css" href="report.css"> 
</head><body 
>
   <div class="maketitle">
                                                                          

                                                                          
                                                                          

                                                                          

<h2 class="titleHead">Compare robustness of nonnegative matrix factorization algorithms</h2>
 <div class="author" ><!--tex4ht:inline--><div class="tabular"> <table id="TBL-2" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1"><col 
id="TBL-2-2"><col 
id="TBL-2-3"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-1"  
class="td11"><span 
class="ptmr8t-x-x-109">Chen Chen</span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-2"  
class="td11"><span 
class="ptmr8t-x-x-109">Xiaodan Gan</span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-2-1-3"  
class="td11"><span 
class="ptmr8t-x-x-109">Xinyue Wang</span></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-1"  
class="td11"><span 
class="ptmr8t-x-x-109">480458339 </span></td> <td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-2"  
class="td11"><span 
class="ptmr8t-x-x-109">440581983 </span></td> <td  style="white-space:nowrap; text-align:left;" id="TBL-2-2-3"  
class="td11"><span 
class="ptmr8t-x-x-109">440359463</span></td></tr></table>                                                                    </div></div>
<br />
<div class="date" ><span 
class="ptmr8t-x-x-109">21st</span><span 
class="ptmr8t-x-x-109">&#x00A0;September 2018</span></div>
   </div><div 
class="abstract" 
>
<div class="center" 
>
<!--l. 1--><p class="noindent" >
<!--l. 1--><p class="noindent" ><span 
class="ptmb8t-x-x-109">Abstract</span></div>
      <!--l. 2--><p class="indent" >      <span 
class="ptmri8t-x-x-109">This  project  numerically  compares  the  robustness  of  two  nonnegative</span>
      <span 
class="ptmri8t-x-x-109">matrix   factorization   (</span><span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span><span 
class="ptmri8t-x-x-109">)   algorithm   [</span><a 
href="#Xlee2001algorithms"><span 
class="ptmri8t-x-x-109">Lee   and   Seung</span></a><span 
class="ptmri8t-x-x-109">,</span><span 
class="ptmri8t-x-x-109">&#x00A0;</span><a 
href="#Xlee2001algorithms"><span 
class="ptmri8t-x-x-109">2001</span></a><span 
class="ptmri8t-x-x-109">]   based</span>
      <span 
class="ptmri8t-x-x-109">on  multiplicative  update  rules  when  contaminated  by  large  magnitude</span>
      <span 
class="ptmri8t-x-x-109">Gaussian,  Poisson  and  Salt  &amp;  Pepper  noise.  Section</span><span 
class="ptmri8t-x-x-109">&#x00A0;</span><a 
href="#x1-30002"><span 
class="ptmri8t-x-x-109">2</span><!--tex4ht:ref: chp3 --></a>  <span 
class="ptmri8t-x-x-109">briefly  review</span>
      <span 
class="ptmri8t-x-x-109">relevant work in the field of  </span><span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span><span 
class="ptmri8t-x-x-109">, which found these multiplicative update</span>
      <span 
class="ptmri8t-x-x-109">rules  are  very  sensitive  to  initialisation.  Section</span><span 
class="ptmri8t-x-x-109">&#x00A0;</span><a 
href="#x1-40003"><span 
class="ptmri8t-x-x-109">3</span><!--tex4ht:ref: chapter2 --></a>  <span 
class="ptmri8t-x-x-109">describes  the  two</span>
      <span 
class="ptmri8t-x-x-109">algorithm and their theoretical properties, proposes a solution to make the</span>
      <span 
class="ptmri8t-x-x-109">multiplicative  update  rules  less  sensitive  to  initialisation,  and  details  the</span>
      <span 
class="ptmri8t-x-x-109">statistical  tools  we  used  to  compare  the  robustness  of  the  two  algorithm.</span>
      <span 
class="ptmri8t-x-x-109">Our  simulation  results  in  Section</span><span 
class="ptmri8t-x-x-109">&#x00A0;</span><a 
href="#x1-140004"><span 
class="ptmri8t-x-x-109">4</span><!--tex4ht:ref: chapter4 --></a>  <span 
class="ptmri8t-x-x-109">agree  with  the  theoretical  properties</span>
      <span 
class="ptmri8t-x-x-109">of  the  two  algorithm.  Future  work  involves  embed  the  proposed  multiple</span>
      <span 
class="ptmri8t-x-x-109">initialisation into the </span><span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">V</span><span 
class="small-caps">D</span><span 
class="small-caps">I</span><span 
class="small-caps">A</span> <span 
class="small-caps">G</span><span 
class="small-caps">P</span><span 
class="small-caps">U</span> </span><span 
class="ptmri8t-x-x-109">based parallel-programming model.</span>
</div>
<a 
 id="Q1-1-1"></a>
<h3 class="likesectionHead"><a 
 id="x1-1000"></a>Contents</h3>
   <div class="tableofcontents">
                                                                          

                                                                          
   <span class="sectionToc" >1 <a 
href="#x1-20001" id="QQ2-1-3">Introduction</a></span>
<br />   <span class="sectionToc" >2 <a 
href="#x1-30002" id="QQ2-1-4">Related work</a></span>
<br />   <span class="sectionToc" >3 <a 
href="#x1-40003" id="QQ2-1-5">Methods </a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.1 <a 
href="#x1-50001" id="QQ2-1-6">NMF and Gaussian noise</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.2 <a 
href="#x1-60002" id="QQ2-1-7">KLNMF and Poisson noise</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.3 <a 
href="#x1-70003" id="QQ2-1-8">Preprocess</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.4 <a 
href="#x1-80004" id="QQ2-1-9">Gaussian and Poisson are asymptotic equivalent</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.5 <a 
href="#x1-90005" id="QQ2-1-11">Salt &amp; Pepper noise</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.6 <a 
href="#x1-100006" id="QQ2-1-12">Multiple initial estimates assure the algorithms stable</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.7 <a 
href="#x1-110007" id="QQ2-1-14">KLNMF requires more iterations</a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.8 <a 
href="#x1-120008" id="QQ2-1-16">Evaluation metrics and their confidence intervals </a></span>
<br />   &#x00A0;<span class="subsectionToc" >3.9 <a 
href="#x1-130009" id="QQ2-1-17">Statistical method compares the robustness of algorithms</a></span>
<br />   <span class="sectionToc" >4 <a 
href="#x1-140004" id="QQ2-1-18">Experiments</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.1 <a 
href="#x1-150001" id="QQ2-1-19">Dataset</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.2 <a 
href="#x1-160002" id="QQ2-1-20">Noise</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.1 <a 
href="#x1-170001" id="QQ2-1-21">Gaussian Noise</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.2 <a 
href="#x1-180002" id="QQ2-1-23">Poisson Noise</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsubsectionToc" >4.2.3 <a 
href="#x1-190003" id="QQ2-1-25">Salt &amp; Pepper Noise</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.3 <a 
href="#x1-200003" id="QQ2-1-27">Experiment Setup</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.4 <a 
href="#x1-210004" id="QQ2-1-28">Experiments Results</a></span>
<br />   &#x00A0;&#x00A0;<span class="subsubsectionToc" >4.4.1 <a 
href="#x1-220001" id="QQ2-1-33">CroppedYale Evaluations metrics FAKE FOR NOW</a></span>
<br />   &#x00A0;<span class="subsectionToc" >4.5 <a 
href="#x1-230005" id="QQ2-1-35">Personal Reflection</a></span>
<br />   <span class="sectionToc" >5 <a 
href="#x1-240005" id="QQ2-1-36">Conclusion</a></span>
   </div>
<a 
 id="x1-1001r1"></a>
<h3 class="sectionHead"><span class="titlemark">1    </span> <a 
 id="x1-20001"></a>Introduction</h3>
<!--l. 3--><p class="noindent" >Non-negative matrix factorization (<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>) is a matrix decomposition technique that
approximates a data matrix with non-negative entries with the multiplication of two
non-negative matrices
   <table 
class="equation-star"><tr><td>
   <center class="math-display" >
<img 
src="report0x.png" alt="V  &#x2248; WH.
                                                                          

                                                                          
" class="math-display" ></center></td></tr></table>
<!--l. 6--><p class="nopar" >
In this approximation, matrix&#x00A0;<span 
class="eurm-10x-x-120">W </span>is the basis and matrix&#x00A0;<span 
class="eurm-10x-x-120">H </span>is the weight matrix
corresponding to the new dictionary&#x00A0;<span 
class="eurm-10x-x-120">W</span>. <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is applicable to an extensive range of
domain. For example, <a 
href="#Xlee1999learning">Lee and Seung</a>&#x00A0;[<a 
href="#Xlee1999learning">1999</a>] suggest that <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is useful for image
processing problems including facial recognition.
<!--l. 10--><p class="indent" >   Matrices&#x00A0;<span 
class="eurm-10x-x-120">W </span>and&#x00A0;<span 
class="eurm-10x-x-120">H </span>are often referred as the basis images and weights. This is
because the observed image <span 
class="eurm-10x-x-120">V </span>is approximated by a linear combination of&#x00A0;<span 
class="eurm-10x-x-120">W </span>with
positive coefficients&#x00A0;<span 
class="eurm-10x-x-120">H</span>. Due to the additive nature of the algorithm, the dictionary&#x00A0;<span 
class="eurm-10x-x-120">W</span>
are often parts of images that are easily interpretable. This property also distinguishes
<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>from other traditional image processing methods such as principal components
analysis (<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">P</span><span 
class="small-caps">C</span><span 
class="small-caps">A</span></span>). <a 
href="#Xguillamet2002non">Guillamet and Vitrià</a>&#x00A0;[<a 
href="#Xguillamet2002non">2002</a>] demonstrate that <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>performs better in
image classification problems in comparison with <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">P</span><span 
class="small-caps">C</span><span 
class="small-caps">A</span></span>. Moreover, <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is also
applicable to text mining such as semantic analysis. Generally, <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is useful to discover
semantic features of an article by counting the frequency of each word, and then
approximating the document from a subset of a large array of features [<a 
href="#Xlee1999learning">Lee and
Seung</a>,&#x00A0;<a 
href="#Xlee1999learning">1999</a>].
<!--l. 18--><p class="indent" >   In practice, face images could be easily corrupted during data collection by noise
with large magnitude. Corruption may result from lighting environment, facial
expression or facial details. An <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithm that is robust to large noise is
desired for real-world application. Therefore, the objective of this project is to
analyse the robustness of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithms on corrupted datasets. We implement
two <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithms proposed by <a 
href="#Xlee2001algorithms">Lee and Seung</a>&#x00A0;[<a 
href="#Xlee2001algorithms">2001</a>] on real face image
datasets, <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">O</span><span 
class="small-caps">R</span><span 
class="small-caps">L</span> </span>dataset and Extended YaleB dataset. We add artificial noises to
the face images are contaminated. We then apply nonparametric statistical
methods to analyse the robustness of these two algorithms from simulation
results.
<a 
 id="x1-2001r3"></a>
<h3 class="sectionHead"><span class="titlemark">2    </span> <a 
 id="x1-30002"></a>Related work</h3>
<!--l. 2--><p class="noindent" >Researchers proposed many <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithms. As the objective function of
<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is non-convex, for which the traditional gradient decent method could be
very sensitive to step sizes and converge slowly, <a 
href="#Xlee2001algorithms">Lee and Seung</a>&#x00A0;[<a 
href="#Xlee2001algorithms">2001</a>] first
proposes to algorithms which minimises Euclidean distance or Kullback-Leibler
divergence&#x00A0;(<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>) between the original matrix and its approximation. Although
these algorithms are easy to implement and have reasonable convergent rate
[<a 
href="#Xlee2001algorithms">Lee and Seung</a>,&#x00A0;<a 
href="#Xlee2001algorithms">2001</a>], they require more iterations than alternatives such as
gradient descent [<a 
href="#Xberry2007algorithms">Berry et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xberry2007algorithms">2007</a>]. Also, the algorithms may fail on seriously
corrupted dataset which violates its assumption of Gaussian noise or Poisson
noise, respectively [<a 
href="#Xguan2017truncated">Guan et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xguan2017truncated">2017</a>]. Moreover, <a 
href="#Xyang2011kullback">Yang et&#x00A0;al.</a>&#x00A0;[<a 
href="#Xyang2011kullback">2011</a>] indicate
that these methods are sensitive to the initial selection of matrices&#x00A0;<span 
class="eurm-10x-x-120">W </span>and&#x00A0;<span 
class="eurm-10x-x-120">H</span>.
                                                                          

                                                                          
The algorithms require many iterations to retrieve from poorly selected initial
values.
<!--l. 4--><p class="indent" >   Apart from different loss functions, several optimization methods were proposed to
improve the performance of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>. After <a 
href="#Xlee2001algorithms">Lee and Seung</a>&#x00A0;[<a 
href="#Xlee2001algorithms">2001</a>] proposed multiplicative
update rules <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">M</span><span 
class="small-caps">U</span><span 
class="small-caps">R</span></span>, <a 
href="#Xlin2007convergence">Lin</a>&#x00A0;[<a 
href="#Xlin2007convergence">2007</a>] proposed a modified <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">M</span><span 
class="small-caps">U</span><span 
class="small-caps">R</span> </span>which guaranteed the
convergence to a stationary point. This modified <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">M</span><span 
class="small-caps">U</span><span 
class="small-caps">R</span></span>, however, did not improve the
convergence rate of traditional <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">M</span><span 
class="small-caps">U</span><span 
class="small-caps">R</span> </span>[<a 
href="#Xguan2012nenmf">Guan et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xguan2012nenmf">2012</a>]. Moreover, as <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">M</span><span 
class="small-caps">U</span><span 
class="small-caps">R</span> </span>does not
impose sparseness, <a 
href="#Xberry2007algorithms">Berry et&#x00A0;al.</a>&#x00A0;[<a 
href="#Xberry2007algorithms">2007</a>] proposed a projected nonnegative least
square (<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">P</span><span 
class="small-caps">N</span><span 
class="small-caps">L</span><span 
class="small-caps">S</span></span>) method to enforce sparseness. In each nonnegative least square
sub-problem, this algorithm projects the negative elements of least squares
solution directly to zero. Nevertheless, <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">P</span><span 
class="small-caps">N</span><span 
class="small-caps">L</span><span 
class="small-caps">S</span> </span>does not guarantee convergence
[<a 
href="#Xguan2012nenmf">Guan et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xguan2012nenmf">2012</a>]. In contrast to these gradient-based optimization methods,
<a 
href="#Xkim2008nonnegative">Kim and Park</a>&#x00A0;[<a 
href="#Xkim2008nonnegative">2008</a>] presented an active set method which divides variables
into two sets, free set and active set. They update free set in each iteration by
solving a unconstrained equation. Even though the active set method has a nice
convergence rate, it assumes strictly convexity in each nonnegative least square
sub-problem [<a 
href="#Xkim2008nonnegative">Kim and Park</a>,&#x00A0;<a 
href="#Xkim2008nonnegative">2008</a>]. These assumptions are easily violated in real life
applications.
<!--l. 6--><p class="indent" >   There exists many robust <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithms which include penalties in the objective
functions. For example, <a 
href="#Xlam2008non">Lam</a>&#x00A0;[<a 
href="#Xlam2008non">2008</a>] proposes <span 
class="eurm-10x-x-120">L</span><sub><span 
class="eurm-10x-x-90">1</span></sub>-norm based <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>to model noisy data
by a Laplace distribution which is less sensitive to outliers. However, as <span 
class="eurm-10x-x-120">L</span><sub><span 
class="eurm-10x-x-90">1</span></sub>-norm is not
differentiable at zero, the optimization procedure is computationally expensive.
<a 
href="#Xkong2011robust">Kong et&#x00A0;al.</a>&#x00A0;[<a 
href="#Xkong2011robust">2011</a>] proposed an <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithm using <span 
class="eurm-10x-x-120">L</span><sub><span 
class="eurm-10x-x-90">21</span></sub>-norm loss function
which is more stable. The updating rules used in <span 
class="eurm-10x-x-120">L</span><sub><span 
class="eurm-10x-x-90">21</span></sub>-norm <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>, however,
still converge slowly because of a continual use of the power method [<a 
href="#Xguan2017truncated">Guan
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xguan2017truncated">2017</a>].
<a 
 id="x1-3001r4"></a>
<h3 class="sectionHead"><span class="titlemark">3    </span> <a 
 id="x1-40003"></a>Methods </h3>
<!--l. 4--><p class="noindent" >Some carefully designed <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>are robust to various noises. These robust algorithms aim
to significantly reduce the amount of noise while preserving the edges without blurring
the immage [<a 
href="#Xbarbu2013variational">Barbu</a>,&#x00A0;<a 
href="#Xbarbu2013variational">2013</a>].
<a 
 id="x1-4001r1"></a>
<h4 class="subsectionHead"><span class="titlemark">3.1    </span> <a 
 id="x1-50001"></a>NMF and Gaussian noise</h4>
<!--l. 7--><p class="noindent" ><span 
class="ptmb8t-x-x-120">Gaussian noise </span>is a noise with a probability density function being normal with mean
zero. <a 
href="#Xlee2001algorithms">Lee and Seung</a>&#x00A0;[<a 
href="#Xlee2001algorithms">2001</a>] propose the first <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>with the objective function between
imaga&#x00A0;<span 
class="eurm-10x-x-120">V </span>and its <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>factorisation&#x00A0;<span 
class="eurm-10x-x-120">W </span>and&#x00A0;<span 
class="eurm-10x-x-120">H </span>being
   <table 
class="equation"><tr><td><a 
 id="x1-5001r1"></a>
                                                                          

                                                                          
   <center class="math-display" >
<img 
src="report1x.png" alt="             &#x2211;  [            ]
&#x2225;V - WH  &#x2225; =     Vij- (WH  )ij2 .
              ij
" class="math-display" ></center></td><td class="equation-label">(1)</td></tr></table>
<!--l. 10--><p class="nopar" >
To minimise this object function of least square, <a 
href="#Xlee2001algorithms">Lee and Seung</a>&#x00A0;[<a 
href="#Xlee2001algorithms">2001</a>] prove the
convergence of the multiplication update rule
   <table 
class="equation"><tr><td><a 
 id="x1-5002r2"></a>
   <center class="math-display" >
<img 
src="report2x.png" alt="           -(WT-V-)jk--               --(VH-)ij---
Hjk &#x2190;  Hjk (WT  WH  )  and Wij &#x2190;  Wij (WHHT   ) .
                    jk                         ij
" class="math-display" ></center></td><td class="equation-label">(2)</td></tr></table>
<!--l. 14--><p class="nopar" >
Here, <span 
class="eufm-10x-x-120">()</span><sub><span 
class="eurm-10x-x-90">ij</span></sub><span 
class="eurm-10x-x-120">&#x2215;</span><span 
class="eufm-10x-x-120">()</span><sub><span 
class="eurm-10x-x-90">ij</span></sub> denotes elementwise division of the two matrix. <a 
href="#Xliu2015performance">Liu and Tao</a>&#x00A0;[<a 
href="#Xliu2015performance">2016</a>]
shows this <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithm minimises Gaussian.
<a 
 id="x1-5003r6"></a>
<h4 class="subsectionHead"><span class="titlemark">3.2    </span> <a 
 id="x1-60002"></a>KLNMF and Poisson noise</h4>
<!--l. 18--><p class="noindent" ><span 
class="ptmb8t-x-x-120">Poisson noise </span>or shot noise is a type of electronic noise that occurs when the finite
number of particles that carry energy, such as electrons in an electronic circuit or
photons in an optical device, is small enough to give rise to detectable statistical
fluctuations in a measurement. <a 
href="#Xlee2001algorithms">Lee and Seung</a>&#x00A0;[<a 
href="#Xlee2001algorithms">2001</a>] suggest that <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is a
algorithm that minimising the Kullback-Leibler divergence <div class="eqnarray">
   <center class="math-display" >
                                                                          

                                                                          
<img 
src="report3x.png" alt="                   (                              )
                &#x2211;             Vij
D (V||WH  )  =        Vijlog (WH---)-- Vij+ (WH  )ij
                 ij               ij
                &#x2211;  (                                  )
            =        -Vijlog (WH  )ij+ (WH  )ij+ C(Vij) .      (3)
                 ij
" class="math-display" ><a 
 id="x1-6001r3"></a></center>
</div>where <span 
class="eurm-10x-x-120">C</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">V</span><sub><span 
class="eurm-10x-x-90">ij</span></sub><span 
class="eufm-10x-x-120">) = </span><span 
class="eurm-10x-x-120">V</span><sub><span 
class="eurm-10x-x-90">ij</span></sub>log<span 
class="eurm-10x-x-120">V</span><sub><span 
class="eurm-10x-x-90">ij</span></sub> <span 
class="eufm-10x-x-120">-</span><span 
class="eurm-10x-x-120">V</span><sub><span 
class="eurm-10x-x-90">ij</span></sub>. <span 
class="eurm-10x-x-120">C</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">V</span><sub><span 
class="eurm-10x-x-90">ij</span></sub><span 
class="eufm-10x-x-120">) </span>is a function of the observed image matrix&#x00A0;<span 
class="eurm-10x-x-120">V</span>
only. <a 
href="#Xlee2001algorithms">Lee and Seung</a>&#x00A0;[<a 
href="#Xlee2001algorithms">2001</a>] also suggest a multiplication update rule to find as the
optimisation procedure of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>
   <table 
class="equation"><tr><td><a 
 id="x1-6002r4"></a>
   <center class="math-display" >
<img 
src="report4x.png" alt="          &#x2211;                                 &#x2211;
          ---iWij&#x2211;Vik&#x2215;-(WH--)jk               ---kHjk&#x2211;Vik-&#x2215;(WH--)jk
Hjk &#x2190;  Hjk        i&#x2032;Wi&#x2032;j      and Wij &#x2190; Wij         k&#x2032;Hik&#x2032;     .
" class="math-display" ></center></td><td class="equation-label">(4)</td></tr></table>
<!--l. 32--><p class="nopar" >
As this original image matrix&#x00A0;<span 
class="eurm-10x-x-120">V </span>is observed, minimising this Kullback-Leibler
divergence&#x00A0;(<a 
href="#x1-6001r3">3<!--tex4ht:ref: eq:klobj --></a>) is equivalent to minimising
   <table 
class="equation-star"><tr><td>
   <center class="math-display" >
<img 
src="report5x.png" alt="&#x2211;  (                                  )
     -V   log (WH  ) + (WH  )  + C(V  ) .
        ij         ij        ij      ij
 ij
" class="math-display" ></center></td></tr></table>
                                                                          

                                                                          
<!--l. 36--><p class="nopar" >
, for arbitrary bounded function&#x00A0;<span 
class="eurm-10x-x-120">C</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">V</span><sub><span 
class="eurm-10x-x-90">ij</span></sub><span 
class="eufm-10x-x-120">)</span>. Taking exponential of the negative of this
score function, the problem transforms to maximising the following likelihood
function
   <table 
class="equation-star"><tr><td>
   <center class="math-display" >
<img 
src="report6x.png" alt="            &#x220F;  (      Vij - (WH )ij        )
L(WH  |V ) =      (WH   )ij e        + C(Vij)  .
             ij
" class="math-display" ></center></td></tr></table>
<!--l. 40--><p class="nopar" >
Choosing constant <span 
class="eurm-10x-x-120">C</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">V</span><sub><span 
class="eurm-10x-x-90">ij</span></sub><span 
class="eufm-10x-x-120">) </span>to be <span 
class="eufm-10x-x-120">-</span>log<span 
class="eurm-10x-x-120">V</span><sub><span 
class="eurm-10x-x-90">ij</span></sub><span 
class="eufm-10x-x-120">! </span>gives
   <table 
class="equation-star"><tr><td>
   <center class="math-display" >
<img 
src="report7x.png" alt="                (                  )
            &#x220F;     (WH  )Vije -(WH )ij
L(WH  |V ) =    ( ------ij----------) .
                         Vij!
             ij
" class="math-display" ></center></td></tr></table>
<!--l. 44--><p class="nopar" >
Hence, the probability density function of each element of the original matrix&#x00A0;V is
Poisson
   <table 
class="equation-star"><tr><td>
   <center class="math-display" >
                                                                          

                                                                          
<img 
src="report8x.png" alt="         (WH  )Vijije -(WH )ij
P(Vij) = -----------------
                Vij!
" class="math-display" ></center></td></tr></table>
<!--l. 48--><p class="nopar" >
is a sufficient condition to yield this likelihood. Hence <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is most suitable for
images with Poisson noise.
<a 
 id="x1-6003r7"></a>
<h4 class="subsectionHead"><span class="titlemark">3.3    </span> <a 
 id="x1-70003"></a>Preprocess</h4>
<!--l. 52--><p class="noindent" >We did not preprocess the images because both of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>and <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>are scale sensitive,
because both objective functions&#x00A0;(<a 
href="#x1-5001r1">1<!--tex4ht:ref: eq:obnmf --></a>) and&#x00A0;(<a 
href="#x1-6001r3">3<!--tex4ht:ref: eq:klobj --></a>) varies when original matrix&#x00A0;<span 
class="eurm-10x-x-120">V </span>and result
matrices&#x00A0;<span 
class="eurm-10x-x-120">W </span>and&#x00A0;<span 
class="eurm-10x-x-120">H </span>scale proportionally, i.e. for <span 
class="eurm-10x-x-120">&#x03BB; </span>real, <span 
class="eurm-10x-x-120">D</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">V</span><span 
class="eusm-10x-x-120">||</span><span 
class="eurm-10x-x-120">WH</span><span 
class="eufm-10x-x-120">)</span><span 
class="eurm-10x-x-120">&#x2260;</span><span 
class="eurm-10x-x-120">D</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">&#x03BB;V</span><span 
class="eusm-10x-x-120">||</span><span 
class="eurm-10x-x-120">&#x03BB;WH</span><span 
class="eufm-10x-x-120">)</span>.
To overcome this issue, we could have normalise the matrices&#x00A0;<span 
class="eurm-10x-x-120">W </span>and&#x00A0;<span 
class="eurm-10x-x-120">H </span>in
each iteration [<a 
href="#Xscaless">Fevotte and Idier</a>,&#x00A0;<a 
href="#Xscaless">2011</a>], but it will result in a even slower
computation.
<a 
 id="x1-7001r8"></a>
<h4 class="subsectionHead"><span class="titlemark">3.4    </span> <a 
 id="x1-80004"></a>Gaussian and Poisson are asymptotic equivalent</h4>
<!--l. 55--><p class="noindent" >We design an Gaussian noise and a Poisson noise with different magnitude. Poisson
distribution with parameter&#x00A0;<span 
class="eurm-10x-x-120">&#x03BB; </span>(integer) is equivalent to the sum of <span 
class="eurm-10x-x-120">&#x03BB; </span>Poisson
distributions with parameter&#x00A0;<span 
class="eurm-10x-x-120">1 </span>[<a 
href="#XWalck:1996cca">Walck</a>,&#x00A0;<a 
href="#XWalck:1996cca">1996</a>, p. 45]. Hence for <span 
class="eurm-10x-x-120">&#x03BB; </span>large, Central Limit
Theorem implies that Poisson distribution with parameter&#x00A0;<span 
class="eurm-10x-x-120">&#x03BB; </span>is well approximated by
<span 
class="eurm-10x-x-120">N</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">&#x03BB;,&#x03BB;</span><span 
class="eufm-10x-x-120">)</span>. When applying Poisson noise to an image, we do not have degree of
freedom to choose any parameter. The variance is the magnitude of the pixels. To
compare the robustness of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>with <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>with different noise, we choose the
variance of Gaussian noise to be the different from the magnitude of the pixel,
that is, <span 
class="eurm-10x-x-120">N</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">0,</span>Var<span 
class="eufm-10x-x-120">)</span><span 
class="eurm-10x-x-120">&#x2260;</span><span 
class="eurm-10x-x-120">N</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">0,V</span><span 
class="eufm-10x-x-120">) </span><span 
class="zptmcm7y-x-x-120">&#x2248;</span> Poi<span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">V</span><span 
class="eufm-10x-x-120">)-</span><span 
class="eurm-10x-x-120">V</span>. Figure&#x00A0;<a 
href="#x1-8001r1">1<!--tex4ht:ref: noise --></a> visualises the similarity
of Poisson distribution and Normal distribution with parameter&#x00A0;<span 
class="eurm-10x-x-120">V </span><span 
class="eufm-10x-x-120">= </span><span 
class="eurm-10x-x-120">40</span>. To
overcome this issue, the Gaussian noise we use should have very different
variance in comparison with the mean of the images. The pixel mean of the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">O</span><span 
class="small-caps">R</span><span 
class="small-caps">L</span></span>
dataset is approximately <span 
class="eurm-10x-x-120">40</span>, and the pixel mean of the Cropped Yale set is
approximately <span 
class="eurm-10x-x-120">70</span>. Hence, we choose the variance of the noise to be <span 
class="eurm-10x-x-120">80</span><sup><span 
class="eurm-10x-x-90">2</span></sup> so
that it is the way bigger than <span 
class="eurm-10x-x-120">255</span>, which is the maximum value of a pixel.
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-8001r1"></a>
                                                                          

                                                                          
<!--l. 64--><p class="noindent" >[scale=.8]noise<br />
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;1: </span><span  
class="content">Compare a Gaussian noise&#x00A0;<span 
class="eurm-10x-x-120">N</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">0,40</span><span 
class="eufm-10x-x-120">) </span>with Poisson noise Poi<span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">40</span><span 
class="eufm-10x-x-120">)-</span><span 
class="eurm-10x-x-120">40</span>.
They two distributions are asymptotically equivalent and have overlapped density
functions. The Gaussian noise&#x00A0;<span 
class="eurm-10x-x-120">N</span><span 
class="eufm-10x-x-120">(</span><span 
class="eurm-10x-x-120">0,80</span><sup><span 
class="eurm-10x-x-90">2</span></sup><span 
class="eufm-10x-x-120">) </span>is very different.</span></div><!--tex4ht:label?: x1-8001r1 -->
                                                                          

                                                                          
<!--l. 66--><p class="indent" >   </div><hr class="endfigure">
<a 
 id="x1-8002r9"></a>
<h4 class="subsectionHead"><span class="titlemark">3.5    </span> <a 
 id="x1-90005"></a>Salt &amp; Pepper noise</h4>
<!--l. 69--><p class="noindent" >Apart from Gaussian and Poisson noises, we also test our two algorithms on
the commonly seen <span 
class="ptmb8t-x-x-120">Salt &amp; Pepper noise </span>noise. The noise presents itself by
having dark pixels in bright regions and bright pixels in dark regions [<a 
href="#Xsampat2005computer">Sampat
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xsampat2005computer">2005</a>].
<a 
 id="x1-9001r11"></a>
<h4 class="subsectionHead"><span class="titlemark">3.6    </span> <a 
 id="x1-100006"></a>Multiple initial estimates assure the algorithms stable</h4>
<!--l. 72--><p class="noindent" >As discussed in the section of related work, The problem of nonnegative matrix
factorization is not a convex problem. Hence the results update rules&#x00A0;(<a 
href="#x1-5002r2">2<!--tex4ht:ref: eq:nmf --></a>) and&#x00A0;(<a 
href="#x1-6002r4">4<!--tex4ht:ref: eq:klnmf --></a>)
coverage to may be local minima instead of global minima, depending on the initial
approximation. Our task was to compare the robustness of the algorithm and we do not
want the instability of our algorithms to affect our comparison. To address this issue, we
implement several (i.e. <span 
class="eurm-10x-x-120">n</span>) initial estimates for each matrix factorization problem. We
use the factorized matrices&#x00A0;<span 
class="eurm-10x-x-120">W </span>and&#x00A0;<span 
class="eurm-10x-x-120">H </span>corresponding to the least residual&#x00A0;(<a 
href="#x1-5001r1">1<!--tex4ht:ref: eq:obnmf --></a>) and&#x00A0;(<a 
href="#x1-6001r3">3<!--tex4ht:ref: eq:klobj --></a>),
for <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>and&#x00A0;<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>, algorithms respectively, as the final result of factorization. This
design of multiple starting point improves the stability of of algorithm, but it requires
more computational power. To improve the computational speed, we make the
number&#x00A0;<span 
class="eurm-10x-x-120">n </span>equal to the number of cores of the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">C</span><span 
class="small-caps">P</span><span 
class="small-caps">U</span></span>. We assign each of the <span 
class="eurm-10x-x-120">n </span>initial
estimates randomly with an uniform distribution. Then these each of the <span 
class="eurm-10x-x-120">n </span>initial
estimates are assigned to a different core of the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">C</span><span 
class="small-caps">P</span><span 
class="small-caps">U</span></span>. This boost the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">C</span><span 
class="small-caps">P</span><span 
class="small-caps">U</span> </span>utilisation to
100% instantly and improved the computational speed by 70% on the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">O</span><span 
class="small-caps">R</span><span 
class="small-caps">L</span> </span>data.
The following part of our code implements this idea of parallel computing.
<!--l. 79-->
<a 
 id="x1-10001r1"></a>
<a 
 id="x1-10002"></a>
<br />
    <div class="caption" 
><span class="id">Algorithm&#x00A0;1:
    </span><span  
class="content">Centring
    image
    data</span></div><!--tex4ht:label?: x1-10001r --><div class="lstlisting" id="listing-1"><span class="label"><a 
 id="x1-10003r1"></a><span 
class="ptmr8t-x-x-60">1</span></span><span 
class="t1-zi4r-0-">args</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">zip</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">repeat</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">,</span><span 
class="t1-zi4r-0-">ncpu</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">repeat</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">r</span><span 
class="t1-zi4r-0-">,</span><span 
class="t1-zi4r-0-">ncpu</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">repeat</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">niter</span><span 
class="t1-zi4r-0-">[</span><span 
class="t1-zi4r-0-">name2</span><span 
class="t1-zi4r-0-">],</span><span 
class="t1-zi4r-0-">ncpu</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">repeat</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">min_error</span><span 
class="t1-zi4r-0-">[</span><span 
class="t1-zi4r-0-">name2</span><span 
class="t1-zi4r-0-">],</span><span 
class="t1-zi4r-0-">ncpu</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-10004r2"></a><span 
class="ptmr8t-x-x-60">2</span></span><span 
class="t1-zi4r-0-">result</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">pool</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">starmap</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">algo</span><span 
class="t1-zi4r-0-">,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">args</span><span 
class="t1-zi4r-0-">)</span>
   </div>
<!--l. 83--><p class="indent" >   where <span 
class="t1-zi4r-0-x-x-120">algo </span>is the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithm and <span 
class="t1-zi4r-0-x-x-120">niter </span>is the number of iterations. We use a
16-thread Xeon high performance computer (Figure&#x00A0;<span 
class="ptmb8t-x-x-120">??</span>) to run this algorithm.
The algorithms run in this high performance computer so that the computing
time is reasonable. The multiple initial estimates assure the algorithms are
stable.
                                                                          

                                                                          
<a 
 id="x1-10005r12"></a>
<h4 class="subsectionHead"><span class="titlemark">3.7    </span> <a 
 id="x1-110007"></a>KLNMF requires more iterations</h4>
<!--l. 95--><p class="noindent" >A residual versus number of iteration plot (figure&#x00A0;<a 
href="#x1-11001r2">2<!--tex4ht:ref: error --></a>) shows that <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>converges
slower than <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>. In the log-log plot, the slope of the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>residual plot is <span 
class="eurm-10x-x-120">2.5 </span>times
larger than that of the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>plot for the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">O</span><span 
class="small-caps">R</span><span 
class="small-caps">L</span> </span>data. This estimates that the rate of
convergence of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is <span 
class="eurm-10x-x-120">2.5 </span>faster than <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>. As a result, we set the number of
iterations as <span 
class="eurm-10x-x-120">500 </span>and <span 
class="eurm-10x-x-120">1200 </span>for <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>and <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithms, respectively (i.e. roughtly
<span 
class="eurm-10x-x-120">2.5 </span>more iterations). <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-11001r2"></a>
                                                                          

                                                                          
<!--l. 99--><p class="noindent" >[scale=.8]error<br />
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;2:  </span><span  
class="content">Residual  of  objective  function&#x00A0;(<a 
href="#x1-5001r1">1<!--tex4ht:ref: eq:obnmf --></a>)  and&#x00A0;(<a 
href="#x1-6001r3">3<!--tex4ht:ref: eq:klobj --></a>)  versus  the  number  of
iterations. <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>converges more than twice faster than <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>.</span></div><!--tex4ht:label?: x1-11001r2 -->
                                                                          

                                                                          
<!--l. 101--><p class="indent" >   </div><hr class="endfigure">
<a 
 id="x1-11002r14"></a>
<h4 class="subsectionHead"><span class="titlemark">3.8    </span> <a 
 id="x1-120008"></a>Evaluation metrics and their confidence intervals </h4>
<!--l. 104--><p class="noindent" >The assignment instruction asks us to compare the performance of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>and <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>by
using evaluations metrics including Relative Reconstruction Errors (<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span></span>), Average
Accuracy (<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">A</span><span 
class="small-caps">A</span></span>), Normalized Mutual Information (<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">I</span></span>). The instruction states the
formulae of these metrics. However, to systematically compare the metrics, we
construct an 95% confidence interval for any metric (e.g. <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span></span>) by bootstrapping
percentile confidence interval. The idea of bootstrapped is straightforward&#8212;we
resample a subset of <span 
class="eurm-10x-x-120">40 </span>samples among the sample space of <span 
class="eurm-10x-x-120">80 </span>Monte-Carlo
simulations and calculate the mean. We repeat this process <span 
class="eurm-10x-x-120">1000 </span>times. The <span 
class="eurm-10x-x-120">2.5</span>% and
<span 
class="eurm-10x-x-120">97.5</span>% percentiles of the <span 
class="eurm-10x-x-120">80 </span>resampled means are then the bootstrapping percentile
confidence interval
   <table 
class="equation"><tr><td><a 
 id="x1-12001r5"></a>
   <center class="math-display" >
<img 
src="report9x.png" alt="    *       *
(RRE2.5, RRE 97.5),
" class="math-display" ></center></td><td class="equation-label">(5)</td></tr></table>
<!--l. 107--><p class="nopar" >
where <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span></span><sub><span 
class="eurm-10x-x-90">&#x03B1;</span></sub><sup><span 
class="zptmcm7y-x-x-90">*</span></sup> is the <span 
class="eurm-10x-x-120">&#x03B1; </span>percentile of the bootstrapped distribution from our sample space
with <span 
class="eurm-10x-x-120">80 </span>Monte-Carlo simulations. We run <span 
class="eurm-10x-x-120">80 </span>simulations so that the confidence interval
we construct is precise.
<!--l. 110--><p class="indent" >   Boostrapping does not require the sample space follows specific distributions. We
apply this nonparametric method to construct confidence interval here because
we do not have the knowledge about the specific distributions of these three
metrics.
<a 
 id="x1-12002r16"></a>
<h4 class="subsectionHead"><span class="titlemark">3.9    </span> <a 
 id="x1-130009"></a>Statistical method compares the robustness of algorithms</h4>
<!--l. 113--><p class="noindent" >We implement Kolmogorov-Smirnov test to test the hypothesis that the algorithms&#x00A0;<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>
and&#x00A0;<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>have different robustness. Again Kolmogorov-Smirnov test is
distribution free, so we do not need to know the distributions of the evaluation
metrics.
                                                                          

                                                                          
<!--l. 115--><p class="indent" >   Let <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span></span><sub><span 
class="eurm-10x-x-90">i</span></sub> denote the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span> </span>generated from our <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithm by the <span 
class="eurm-10x-x-120">i</span>th Monte-Carlo
simulation. Define the empirical distribution of a sample set generated by algorithm&#x00A0;<span 
class="eurm-10x-x-120">&#x03B1;</span>,
perhaps the <span 
class="eurm-10x-x-120">80 </span><span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span> </span>results, as
   <table 
class="equation"><tr><td><a 
 id="x1-13001r6"></a>
   <center class="math-display" >
<img 
src="report10x.png" alt="         1&#x2211;80
^F&#x03B1; (x ) =--    1RREi&#x2264;x.
        n  i=1
" class="math-display" ></center></td><td class="equation-label">(6)</td></tr></table>
<!--l. 118--><p class="nopar" >
The test statistic is the supremum among the differences of the empirical distribution
generated using definition&#x00A0;(<a 
href="#x1-13001r6">6<!--tex4ht:ref: epdf --></a>) [<a 
href="#XWalck:1996cca">Walck</a>,&#x00A0;<a 
href="#XWalck:1996cca">1996</a>]
   <table 
class="equation"><tr><td><a 
 id="x1-13002r7"></a>
   <center class="math-display" >
<img 
src="report11x.png" alt="D = sup |F&#x03B1; (x) - F&#x03B1; (x)|.
     x    2        2
" class="math-display" ></center></td><td class="equation-label">(7)</td></tr></table>
<!--l. 122--><p class="nopar" >
We compare the test statistics&#x00A0;<span 
class="eurm-10x-x-120">D </span>with the critical value of <span 
class="eurm-10x-x-120">0.215</span>, which corresponds to
<span 
class="eurm-10x-x-120">80 </span>samples and a 95% of confidence level. We reject the null hypotheses that the two
algorithms produce similar <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span> </span>(or other evaluation metrics) with 95% confidence level
if the test statistics&#x00A0;<span 
class="eurm-10x-x-120">D &#x003E; 0.215</span>. The technique is extended to compare <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">A</span><span 
class="small-caps">A</span> </span>and
<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">I</span></span>.
<a 
 id="x1-13003r5"></a>
<h3 class="sectionHead"><span class="titlemark">4    </span> <a 
 id="x1-140004"></a>Experiments</h3>
<a 
 id="x1-14001r17"></a>
                                                                          

                                                                          
<h4 class="subsectionHead"><span class="titlemark">4.1    </span> <a 
 id="x1-150001"></a>Dataset</h4>
<!--l. 4--><p class="noindent" >We illustrate our two <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithms on two real-world face image datasets: ORL
and Extended YaleB (<a 
href="#Xbelhumeur1997eigenfaces">Belhumeur et&#x00A0;al.</a>&#x00A0;[<a 
href="#Xbelhumeur1997eigenfaces">1997</a>]). Both ORL and Extended
YaleB datasets contain multiple images of distinct subjects with various facial
expression, lighting condition, and facial details. Images in ORL are cropped and
resized to <span 
class="eurm-10x-x-120">92</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="eurm-10x-x-120">112 </span>pixels. We further rescale it to <span 
class="eurm-10x-x-120">30</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="eurm-10x-x-120">37 </span>pixels. Similarly, we
reduce the size of images in Extended YaleB to <span 
class="eurm-10x-x-120">42</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="eurm-10x-x-120">48 </span>pixels. For each dataset,
we flatten the image matrix into a vector and append them together to get a
matrix <span 
class="eurm-10x-x-120">V </span>with shape <span 
class="eurm-10x-x-120">d</span><span 
class="zptmcm7y-x-x-120">&#x00D7;</span><span 
class="eurm-10x-x-120">n </span>where integer&#x00A0;<span 
class="eurm-10x-x-120">d </span>is the number of pixels in one
image and integer&#x00A0;<span 
class="eurm-10x-x-120">n </span>is the number of images. In each epoch, we use 90% of
data.
<a 
 id="x1-15001r19"></a>
<h4 class="subsectionHead"><span class="titlemark">4.2    </span> <a 
 id="x1-160002"></a>Noise</h4>
<!--l. 10--><p class="noindent" >We implement three kinds of noises including Gaussian noise, Poisson noise and Salt &amp;
Pepper noise.
<a 
 id="x1-16001r1"></a>
<h5 class="subsubsectionHead"><span class="titlemark">4.2.1    </span> <a 
 id="x1-170001"></a>Gaussian Noise</h5>
<!--l. 12--><p class="noindent" >We design the Gaussian noise by normal distribution with <span 
class="eurm-10x-x-120">0 </span>mean and <span 
class="eurm-10x-x-120">80 </span>standard
deviation (Algorithm&#x00A0;<a 
href="#x1-17001r2">2<!--tex4ht:ref: gau --></a>). The <span 
class="t1-zi4r-0-x-x-120">ORL </span>dataset has a global pixel mean of <span 
class="eurm-10x-x-120">40 </span>and the
<span 
class="t1-zi4r-0-x-x-120">CroppedYale </span>data set has that of <span 
class="eurm-10x-x-120">70</span>. Hence the designed Gaussian noise contaminates
the images significantly. We choose the standard deviation to be <span 
class="eurm-10x-x-120">80 </span>so that our Gaussian
noise are less likely to coincident with the designed Poisson noise. To satisfy the
nonnegative constant, negative value in contaminated image is set to zero.
<!--l. 13-->
<a 
 id="x1-17001r2"></a>
<a 
 id="x1-17002"></a>
<br />
        <div class="caption" 
><span class="id">Algorithm&#x00A0;2:
        </span><span  
class="content">Gaussian
        Noise
        Design</span></div><!--tex4ht:label?: x1-17001r --><div class="lstlisting" id="listing-2"><span class="label"><a 
 id="x1-17003r1"></a><span 
class="ptmr8t-x-x-60">1</span></span><span 
class="t1-zi4r-0-">def</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">normal</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">subVhat</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">:</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-17004r2"></a><span 
class="ptmr8t-x-x-60">2</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">"""</span><span 
class="t1-zi4r-0-">Design</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">a</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">Gaussian</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">noise</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">"""</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-17005r3"></a><span 
class="ptmr8t-x-x-60">3</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V_noise</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">np</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">random</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">normal</span><span 
class="t1-zi4r-0-">(0,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">80,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">subVhat</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">shape</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">#</span><span 
class="t1-zi4r-0-">*</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">np</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">sqrt</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">subVhat</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-17006r4"></a><span 
class="ptmr8t-x-x-60">4</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">subVhat</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">+</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V_noise</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-17007r5"></a><span 
class="ptmr8t-x-x-60">5</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">[</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-"><</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">0]</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">0</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-17008r6"></a><span 
class="ptmr8t-x-x-60">6</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">return</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V_noise</span>
   </div>
<a 
 id="x1-17009r21"></a>
<h5 class="subsubsectionHead"><span class="titlemark">4.2.2    </span> <a 
 id="x1-180002"></a>Poisson Noise</h5>
<!--l. 24--><p class="noindent" >The Poisson noise is not additive and has no hyperparameters to be set. Unlike Gaussian
noise, contaminated images are drawn directly from Poisson distribution with parameter
set to be pixel values. Then, the Poisson noise is calculated from the difference between
the contaminated image and the original image, as discussed in Section&#x00A0;<a 
href="#x1-40003">3<!--tex4ht:ref: chapter2 --></a> and
                                                                          

                                                                          
demonstrated in algorithm&#x00A0;<a 
href="#x1-18001r3">3<!--tex4ht:ref: poi --></a>. <!--l. 25-->
<a 
 id="x1-18001r3"></a>
<a 
 id="x1-18002"></a>
<br />
        <div class="caption" 
><span class="id">Algorithm&#x00A0;3:
        </span><span  
class="content">Poisson
        Noise
        Design</span></div><!--tex4ht:label?: x1-18001r --><div class="lstlisting" id="listing-3"><span class="label"><a 
 id="x1-18003r1"></a><span 
class="ptmr8t-x-x-60">1</span></span><span 
class="t1-zi4r-0-">def</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">possion</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">subVhat</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">:</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-18004r2"></a><span 
class="ptmr8t-x-x-60">2</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">"""</span><span 
class="t1-zi4r-0-">Design</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">a</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">Possion</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">noise</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">"""</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-18005r3"></a><span 
class="ptmr8t-x-x-60">3</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">np</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">random</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">poisson</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">subVhat</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-18006r4"></a><span 
class="ptmr8t-x-x-60">4</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V_noise</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">-</span><span 
class="t1-zi4r-0-">subVhat</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-18007r5"></a><span 
class="ptmr8t-x-x-60">5</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">return</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V_noise</span>
   </div>
<a 
 id="x1-18008r23"></a>
<h5 class="subsubsectionHead"><span class="titlemark">4.2.3    </span> <a 
 id="x1-190003"></a>Salt &amp; Pepper Noise</h5>
<!--l. 36--><p class="noindent" >Salt &amp; Pepper noises are added by drawing random integers from discrete
uniform distribution of the interval <span 
class="eufm-10x-x-120">[</span>0, 255<span 
class="eufm-10x-x-120">) </span>. We find bright place in generated
image and replace pixel values in same place of original image with brightest
value. Similarly, we also find the dark pixels in generated image and replace
pixel values in same place of original image with darkest pixel value. In this
case, we set the pixels whose value being greater than or equal to 230 as bright
pixels and pixels whose value being less than or equal to 20 as dark pixels.
<!--l. 37-->
<a 
 id="x1-19001r4"></a>
<a 
 id="x1-19002"></a>
<br />
      <div class="caption" 
><span class="id">Algorithm&#x00A0;4:
      </span><span  
class="content">Salt
      and
      Pepper
      Noise
      Design</span></div><!--tex4ht:label?: x1-19001r --><div class="lstlisting" id="listing-4"><span class="label"><a 
 id="x1-19003r1"></a><span 
class="ptmr8t-x-x-60">1</span></span><span 
class="t1-zi4r-0-">def</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">salt_and_pepper</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">subVhat</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">:</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-19004r2"></a><span 
class="ptmr8t-x-x-60">2</span></span><span 
class="t1-zi4r-0-">"""</span><span 
class="t1-zi4r-0-">Design</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">a</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">salt</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">and</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">pepper</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">noise</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">where</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">make</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">some</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">pixel</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">value</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">zeros</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">"""</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-19005r3"></a><span 
class="ptmr8t-x-x-60">3</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V_noise</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">np</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">random</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">randint</span><span 
class="t1-zi4r-0-">(</span><span 
class="t1-zi4r-0-">low</span><span 
class="t1-zi4r-0-">=0,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">high</span><span 
class="t1-zi4r-0-">=255,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">size</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">subVhat</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">shape</span><span 
class="t1-zi4r-0-">,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">dtype</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">int</span><span 
class="t1-zi4r-0-">)</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-19006r4"></a><span 
class="ptmr8t-x-x-60">4</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">subVhat</span><span 
class="t1-zi4r-0-">.</span><span 
class="t1-zi4r-0-">copy</span><span 
class="t1-zi4r-0-">()</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-19007r5"></a><span 
class="ptmr8t-x-x-60">5</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">[</span><span 
class="t1-zi4r-0-">V_noise</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-"><=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">20]</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">0</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-19008r6"></a><span 
class="ptmr8t-x-x-60">6</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">[</span><span 
class="t1-zi4r-0-">V_noise</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">>=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">230]</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">=</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">255</span><span 
class="t1-zi4r-0-">&#x00A0;</span><br /><span class="label"><a 
 id="x1-19009r7"></a><span 
class="ptmr8t-x-x-60">7</span></span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">return</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V</span><span 
class="t1-zi4r-0-">,</span><span 
class="t1-zi4r-0-">&#x00A0;</span><span 
class="t1-zi4r-0-">V_noise</span>
   </div>
<a 
 id="x1-19010r20"></a>
<h4 class="subsectionHead"><span class="titlemark">4.3    </span> <a 
 id="x1-200003"></a>Experiment Setup</h4>
<!--l. 49--><p class="noindent" >We apply two algorithms (<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>and <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>) with four categories of noises (no noise,
Gaussian noise, Poisson noise and Salt &amp; Pepper noise), which results in eight
combinations in each epoch. In each epoch, we randomly select 90% of samples to train
NMF algorithms and evaluate three metrics on reconstructed images. The training will
terminate when the error reaches the minimum error, or the maximum iteration is
reached. The minimum error and maximum iteration are hyperparameters which we
learn from iterative experiments. Our code saves the learning errors versus number of
iterations so that we could draw the plot and observe the convergence of learning
                                                                          

                                                                          
process. We increase the number of epochs and calculate the average metrics and
confidence intervals.
<a 
 id="x1-20001r27"></a>
<h4 class="subsectionHead"><span class="titlemark">4.4    </span> <a 
 id="x1-210004"></a>Experiments Results</h4>
<!--l. 53--><p class="noindent" >Figure&#x00A0;<a 
href="#x1-21001r3">3<!--tex4ht:ref: noisesnmff --></a> and&#x00A0;<a 
href="#x1-21002r4">4<!--tex4ht:ref: noisesklnmff --></a> visualise the original image, designed noises, corrupted images and
reconstructed images from left to right. From top to bottom, the four rows correspond to
no noise, Gaussian noise discussed in Section&#x00A0;<a 
href="#x1-170001">4.2.1<!--tex4ht:ref: sec:gau --></a>, Poisson noise discussed in
Section&#x00A0;<a 
href="#x1-180002">4.2.2<!--tex4ht:ref: sec:poi --></a> and Salt &amp; Paper noise discussed in Section&#x00A0;<a 
href="#x1-190003">4.2.3<!--tex4ht:ref: sec:sal --></a>. The first row of
Figure&#x00A0;<a 
href="#x1-21001r3">3<!--tex4ht:ref: noisesnmff --></a> and&#x00A0;<a 
href="#x1-21002r4">4<!--tex4ht:ref: noisesklnmff --></a> shows both algorithms reconstructed the original image well without
artificial noise and with Poisson noise. <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-21001r3"></a>
                                                                          

                                                                          
<!--l. 57--><p class="noindent" >[scale=.9]Result_Multiplication_Euclidean_No_Noise_Comparison<br />
[scale=.9]Result_Multiplication_Euclidean_Normal_Comparison<br />
[scale=.9]Result_Multiplication_Euclidean_Poisson_Comparison<br />
[scale=.9]Result_Multiplication_Euclidean_Salt_and_Pepper_Comparison
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;3: </span><span  
class="content">The reconstructed image by <span 
class="ptmb8t-x-x-120">nmf</span>. The original images (Column&#x00A0;1) are
combined with noises (Column&#x00A0;1) including Gaussian Noise with Variance&#x00A0;<span 
class="eurm-10x-x-120">80</span>
(Row&#x00A0;2),  Poisson  Noise  (Row&#x00A0;3),  and  Salt  &amp;  Pepper  Noise  (Row&#x00A0;4).  The
corrupted images are shown in Column&#x00A0;3. The reconstructed images are shown
in (Column&#x00A0;4). The reconstruction with no noise is shown in Row&#x00A0;1.</span></div><!--tex4ht:label?: x1-21001r3 -->
                                                                          

                                                                          
<!--l. 62--><p class="indent" >   </div><hr class="endfigure">
<!--l. 63--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-21002r4"></a>
                                                                          

                                                                          
<!--l. 65--><p class="noindent" >[scale=.9]Result_Multiplication_KL_Divergence_No_Noise_Comparison<br />
[scale=.9]Result_Multiplication_KL_Divergence_Normal_Comparison<br />
[scale=.9]Result_Multiplication_KL_Divergence_Poisson_Comparison<br />
[scale=.9]Result_Multiplication_KL_Divergence_Salt_and_Pepper_Comparison
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;4: </span><span  
class="content">The reconstructed image by <span 
class="ptmb8t-x-x-120">klnmf</span>. The original images (Column&#x00A0;1)
are   combined   with   noises   (Column&#x00A0;1)   including   Gaussian   Noise   with
Variance&#x00A0;<span 
class="eurm-10x-x-120">80  </span>(Row&#x00A0;2),  Poisson  Noise  (Row&#x00A0;3),  and  Salt  &amp;  Pepper  Noise
(Row&#x00A0;4).  The  corrupted  images  are  shown  in  Column&#x00A0;3.  The  reconstructed
images are shown in (Column&#x00A0;4). The reconstruction with no noise is shown in
Row&#x00A0;1.</span></div><!--tex4ht:label?: x1-21002r4 -->
                                                                          

                                                                          
<!--l. 70--><p class="indent" >   </div><hr class="endfigure">
<!--l. 71--><p class="indent" >   However, when the noise is large (the second and last rows in Figure&#x00A0;<a 
href="#x1-21001r3">3<!--tex4ht:ref: noisesnmff --></a> and&#x00A0;<a 
href="#x1-21002r4">4<!--tex4ht:ref: noisesklnmff --></a>), the
quality of reconstructed images looks marginally better than the contaminated images.
This result is consistent with <a 
href="#Xguan2017truncated">Guan et&#x00A0;al.</a>&#x00A0;[<a 
href="#Xguan2017truncated">2017</a>], who assert that <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>may fail to
handle extremely corrupted images when assumed distribution of noise is violated.
Moreover, the difference between images generated by <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>and <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>are not
visually significant. Hence, we implement statistical hypothesis test to compare of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span></span>s
of the two algorithms.
<!--l. 76--><p class="indent" >   Substitute <a 
href="https://raw.githubusercontent.com/JoyceXinyueWang/nmf_raw_data/master/raw_result_acc.csv" ><span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span> </span>results</a> (Figure&#x00A0;<a 
href="#x1-21003r5">5<!--tex4ht:ref: histo --></a>) from <span 
class="eurm-10x-x-120">80 </span>Monte-Carlo simulations of the two
algorithms into Kolmogorov-Smirnovs test&#x00A0;(<a 
href="#x1-13002r7">7<!--tex4ht:ref: teststatistic --></a>) gives test statistics&#x00A0;<span 
class="eurm-10x-x-120">D </span><span 
class="eufm-10x-x-120">= </span><span 
class="eurm-10x-x-120">1,1,0.6625</span>
with no noise, Gaussian noise, and Poisson noise, for the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">O</span><span 
class="small-caps">R</span><span 
class="small-caps">L</span> </span>dataset. These three test
statistics are all much greater than the critical value&#x00A0;<span 
class="eurm-10x-x-120">0.215</span>. Hence there are strong
evidence that the performance of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>and <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>are different in these three problems.
For salt and Pepper noise, test statistic&#x00A0;<span 
class="eurm-10x-x-120">D </span><span 
class="eufm-10x-x-120">= </span><span 
class="eurm-10x-x-120">0.2125 &#x003C; 0.215</span>, hence we fail to conclude
that the two methods have different robustness against Salt and Pepper noise.
Further one tail Kolmogorov-Smirnov test concludes that <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>performs better
reconstructing the original image with no noise and is more robust Poisson noise.
<hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x1-21003r5"></a>
                                                                          

                                                                          
<!--l. 79--><p class="noindent" >[scale=0.8]histo
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;5: </span><span  
class="content">Histogram of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span> </span>results from 80 Monte-Carlo simulations. Blue bars
correspond to <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>and the pink bars correspond to <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>. The type of noise are
labeled in the plot. The visualisation agrees with our statistical analysis.</span></div><!--tex4ht:label?: x1-21003r5 -->
                                                                          

                                                                          
<!--l. 82--><p class="indent" >   </div><hr class="endfigure">
<!--l. 84--><p class="indent" >   In contrast, <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is more robust against Gaussian noise, even with only <span 
class="eurm-10x-x-120">500</span>
iterations (<span 
class="eurm-10x-x-120">1200 </span>for <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>). Hence, <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is clearly more robust than <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>against
Gaussian noise.
<!--l. 86--><p class="indent" >   Theoretical results discussed in Section&#x00A0;<a 
href="#x1-40003">3<!--tex4ht:ref: chapter2 --></a> suggest <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is more robust against
Gaussian noise whereas <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>is more robust against Poisson noise. Our
experimental results concluded from Kolmogorov-Smirnovs hypothesis tests
agree with these theoretical results. Further, as both of the algorithms are not
designed for Salt and Pepper noise, they have similar performance against
it.
<!--l. 88--><p class="indent" >   These results can be observed by directly reading whether the confidence intervals
overlap in table&#x00A0;<a 
href="#x1-21004r1">1<!--tex4ht:ref: tab:ci --></a>. Also, the statistical results agree with the visualisation in
Figure&#x00A0;<a 
href="#x1-21003r5">5<!--tex4ht:ref: histo --></a>,&#x00A0;<a 
href="#x1-21001r3">3<!--tex4ht:ref: noisesnmff --></a> and&#x00A0;<a 
href="#x1-21002r4">4<!--tex4ht:ref: noisesklnmff --></a>. These results also agree with our intuition&#8212;although the
differences in the robustness of the two algorithms are small, the even smaller variances
in the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span> </span>results make them statistically different under Poisson and Gaussian noise
(Figure&#x00A0;<a 
href="#x1-21003r5">5<!--tex4ht:ref: histo --></a>).
<!--l. 90--><p class="indent" >   In terms of the cropped Yale dataset, results from hypothesis tests show that <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>
performs uniformly better than <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>, suggesting more iterations are required on
<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>to compare these two algorithms fairly. We fail to do so because the dataset is
much larger than <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">O</span><span 
class="small-caps">R</span><span 
class="small-caps">L</span> </span>and our multiplicative update rules, especially for <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>
converge too slow.
   <div class="table">
                                                                          

                                                                          
<!--l. 93--><p class="indent" >   <a 
 id="x1-21004r1"></a><hr class="float"><div class="float" 
>
                                                                          

                                                                          
 <div class="caption" 
><span class="id">Table&#x00A0;1: </span><span  
class="content">Average of evaluations metrics over 80 Monte-Carlo simulations using
the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">O</span><span 
class="small-caps">R</span><span 
class="small-caps">L</span> </span>dataset. The 95% confidence intervals are calculated using bootstrap.</span></div><!--tex4ht:label?: x1-21004r1 -->
<!--tex4ht:inline--><div class="tabular"><table id="TBL-3" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-3-1g"><col 
id="TBL-3-1"></colgroup><colgroup id="TBL-3-2g"><col 
id="TBL-3-2"><col 
id="TBL-3-3"><col 
id="TBL-3-4"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">O</span><span 
class="small-caps">R</span><span 
class="small-caps">L</span> </span><span 
class="ptmr8t-x-x-109">dataset                              </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-2"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span>                                  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-3"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">A</span><span 
class="small-caps">C</span><span 
class="small-caps">C</span>                                  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-4"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">I</span>                                  </span></td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">no noise </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.1583 (0.1581, 0.1584) </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.7364 (0.731, 0.742) </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.8536 (0.8506, 0.8567)</span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Gaussian noise                 </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.2925 (0.2922, 0.2927)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.447 (0.4423, 0.4521)    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.6212 (0.6176, 0.6247)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Poisson noise                    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.1611 (0.161, 0.1613)    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.7313 (0.7262, 0.7367)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.8493 (0.8456, 0.8527)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Salt and Pepper noise       </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.2636 (0.2634, 0.2638)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.5094 (0.504, 0.5151)    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.6721 (0.6679, 0.6764)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">no noise                       </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.1729 (0.1728, 0.173)    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.7406 (0.7352, 0.7458)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.8599 (0.8568, 0.8632)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-7-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Gaussian noise             </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-7-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.2977 (0.2976, 0.2979)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-7-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.4538 (0.4483, 0.4595)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-7-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.6209 (0.6165, 0.6255)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-8-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Poisson noise               </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-8-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.1602 (0.1601, 0.1603)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-8-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.7417 (0.7365, 0.7472)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-8-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.8573 (0.8542, 0.8602)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-9-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Salt and Pepper noise  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-9-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.264 (0.2638, 0.2643)    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-9-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.5089 (0.5038, 0.5139)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-9-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.6734 (0.6694, 0.6779)  </span></td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-10-1"  
class="td11">                          </td></tr></table></div>
                                                                          

                                                                          
   </div><hr class="endfloat" />
   </div>
<a 
 id="x1-21005r25"></a>
<h5 class="subsubsectionHead"><span class="titlemark">4.4.1    </span> <a 
 id="x1-220001"></a>CroppedYale Evaluations metrics FAKE FOR NOW</h5>
   <div class="table">
                                                                          

                                                                          
<!--l. 120--><p class="indent" >   <a 
 id="x1-22001r2"></a><hr class="float"><div class="float" 
>
                                                                          

                                                                          
 <div class="caption" 
><span class="id">Table&#x00A0;2:  </span><span  
class="content">Average  of  evaluations  metrics  over  40  Monte-Carlo  simulations
using CroppedYale data set. The 95% confidence intervals are calculated using
bootstrap.</span></div><!--tex4ht:label?: x1-22001r2 -->
<!--tex4ht:inline--><div class="tabular"><table id="TBL-4" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-4-1g"><col 
id="TBL-4-1"></colgroup><colgroup id="TBL-4-2g"><col 
id="TBL-4-2"><col 
id="TBL-4-3"><col 
id="TBL-4-4"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">O</span><span 
class="small-caps">R</span><span 
class="small-caps">L</span> </span><span 
class="ptmr8t-x-x-109">dataset                              </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-2"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span>                                  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-3"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">A</span><span 
class="small-caps">C</span><span 
class="small-caps">C</span>                                  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-1-4"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">I</span>                                  </span></td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">no noise </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.1583 (0.1581, 0.1584) </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.7364 (0.731, 0.742) </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-2-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.8536 (0.8506, 0.8567)</span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-4-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Gaussian noise                 </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.2925 (0.2922, 0.2927)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.447 (0.4423, 0.4521)    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-3-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.6212 (0.6176, 0.6247)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-4-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Poisson noise                    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.1611 (0.161, 0.1613)    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.7313 (0.7262, 0.7367)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-4-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.8493 (0.8456, 0.8527)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-4-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Salt and Pepper noise       </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.2636 (0.2634, 0.2638)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.5094 (0.504, 0.5151)    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-5-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.6721 (0.6679, 0.6764)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-4-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">no noise                       </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.1729 (0.1728, 0.173)    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.7406 (0.7352, 0.7458)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-6-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.8599 (0.8568, 0.8632)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-4-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-7-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Gaussian noise             </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-7-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.2977 (0.2976, 0.2979)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-7-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.4538 (0.4483, 0.4595)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-7-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.6209 (0.6165, 0.6255)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-4-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-8-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Poisson noise               </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-8-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.1602 (0.1601, 0.1603)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-8-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.7417 (0.7365, 0.7472)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-8-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.8573 (0.8542, 0.8602)  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-4-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-9-1"  
class="td11"> <span 
class="ptmrc8t-x-x-109"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><span 
class="ptmr8t-x-x-109">Salt and Pepper noise  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-9-2"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.264 (0.2638, 0.2643)    </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-9-3"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.5089 (0.5038, 0.5139)  </span></td><td  style="white-space:nowrap; text-align:left;" id="TBL-4-9-4"  
class="td11"> <span 
class="ptmr8t-x-x-109">0.6734 (0.6694, 0.6779)  </span></td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-4-10-1"  
class="td11">                          </td></tr></table></div>
                                                                          

                                                                          
   </div><hr class="endfloat" />
   </div>
<a 
 id="x1-22002r28"></a>
<h4 class="subsectionHead"><span class="titlemark">4.5    </span> <a 
 id="x1-230005"></a>Personal Reflection</h4>
<!--l. 145--><p class="noindent" >The implementation of this project was challenging and rewarding. We overcome many
problems that are not taught in class by research. For instance, we learnt to use parallel
computing when the performance of&#x00A0;<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>was always struggling due to local
optimal. Moreover, we critically considered what truly defines a &#8216;good&#8217; algorithm.
Although theoretically privileged algorithms may be good for handling difficult tasks,
they tend to be time consuming and not easy to implement. For example, through
literature review, we found some algorithms such as Truncated Cauchy <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span><a 
href="#Xguan2017truncated">Guan
et&#x00A0;al.</a>&#x00A0;[<a 
href="#Xguan2017truncated">2017</a>] that are excellent for contaminated data but too difficult for us to
implement. We also suffered time-performance tradeoff especially when training
Extended YaleB dataset. Hence, in real-world practice, simpler and faster algorithm
may be more widely used than advanced algorithms. Lastly, we observed many
interesting results during the experiment. For instance, we found that the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">R</span><span 
class="small-caps">R</span><span 
class="small-caps">E</span></span>s of <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>
and <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>with Poisson noise is superior than that with no noise. One hypothesis is
that added Poisson noise neutralized the noise from original image. Another
hypothesis is that this may result from the noise assumptions made by these two
algorithms.
<a 
 id="x1-23001r18"></a>
<h3 class="sectionHead"><span class="titlemark">5    </span> <a 
 id="x1-240005"></a>Conclusion</h3>
<!--l. 2--><p class="noindent" >In conclusion, our numerical simulation supports the theoretical results that <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>based
on objective function&#x00A0;(<a 
href="#x1-5001r1">1<!--tex4ht:ref: eq:obnmf --></a>) is more robust to Gaussian noise and <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>based on
objective function&#x00A0;(<a 
href="#x1-6001r3">3<!--tex4ht:ref: eq:klobj --></a>) is more robust to Poisson noise. The two algorithms have similar
performance against Pepper &amp; Salt noise. However, the <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span> </span>algorithm reconstruct
image better without noise and converges much faster with multiplicative update rule
when comparing with <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">K</span><span 
class="small-caps">L</span><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>. Also, we found the multiplicative update rule is
sensitive to initial value of matrices&#x00A0;<span 
class="eurm-10x-x-120">W </span>and&#x00A0;<span 
class="eurm-10x-x-120">H</span>. We proposed a solution based on
parallel programming to solve this problem. However, this solution requires high
performance computer so that the algorithm converges in a reasonable amount of
time.
<!--l. 4--><p class="indent" >   Recently, <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">V</span><span 
class="small-caps">I</span><span 
class="small-caps">D</span><span 
class="small-caps">I</span><span 
class="small-caps">A</span> </span>released their <a 
href="https://developer.nvidia.com/cuda-zone" ><span 
class="t1-zi4r-0-x-x-120">Cuda</span></a> package which parallelise algorithms using
<span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">G</span><span 
class="small-caps">P</span><span 
class="small-caps">U</span></span>. This package improves the speed of parallelised algorithms, including many <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">N</span><span 
class="small-caps">M</span><span 
class="small-caps">F</span></span>
algorithms, by a factor of <span 
class="eurm-10x-x-120">&#x003E; 100</span>. As a computational expensive procedure, our
suggestion of using multiple initialisation will be more novel if a <span 
class="ptmrc8t-x-x-120"><span 
class="small-caps">G</span><span 
class="small-caps">P</span><span 
class="small-caps">U</span> </span>version based on
<a 
href="https://developer.nvidia.com/cuda-zone" ><span 
class="t1-zi4r-0-x-x-120">Cuda</span></a> could be made.
                                                                          

                                                                          
<a 
 id="Q1-1-37"></a>
<h3 class="likesectionHead"><a 
 id="x1-250005"></a>References</h3>
  <div class="thebibliography">
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlee2001algorithms"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Daniel&#x00A0;D.     Lee     and     H.&#x00A0;Sebastian     Seung.               Algorithms     for
  non-negative      matrix      factorization.                In      T.&#x00A0;K.      Leen,      T.&#x00A0;G.
  Dietterich,    and    V.&#x00A0;Tresp,    editors,    <span 
class="ptmri8t-x-x-120">Advances    in    Neural    Information</span>
  <span 
class="ptmri8t-x-x-120">Processing   Systems   13</span>,   pages   556&#8211;562.   MIT   Press,   2001.         URL
  <a 
href="http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf" class="url" ><span 
class="t1-zi4r-0-x-x-120">http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlee1999learning"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Daniel&#x00A0;D  Lee  and  H&#x00A0;Sebastian  Seung.    Learning  the  parts  of  objects  by
  non-negative matrix factorization. <span 
class="ptmri8t-x-x-120">Nature</span>, 401(6755):788, 1999.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xguillamet2002non"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>David Guillamet and Jordi Vitrià.  Non-negative matrix factorization for face
  recognition. In <span 
class="ptmri8t-x-x-120">Topics in artificial intelligence</span>, pages 336&#8211;344. Springer, 2002.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xberry2007algorithms"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Michael&#x00A0;W  Berry,  Murray  Browne,  Amy&#x00A0;N  Langville,  V&#x00A0;Paul  Pauca,  and
  Robert&#x00A0;J Plemmons. Algorithms and applications for approximate nonnegative
  matrix factorization. <span 
class="ptmri8t-x-x-120">Computational statistics &amp; data analysis</span>, 52(1):155&#8211;173,
  2007.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xguan2017truncated"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Naiyang   Guan,   Tongliang   Liu,   Yangmuzi   Zhang,   Dacheng   Tao,   and
  Larry&#x00A0;Steven Davis.   Truncated cauchy non-negative matrix factorization for
  robust subspace learning. <span 
class="ptmri8t-x-x-120">IEEE Transactions on Pattern Analysis and Machine</span>
  <span 
class="ptmri8t-x-x-120">Intelligence</span>, 2017.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xyang2011kullback"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Zhirong  Yang,  He&#x00A0;Zhang,  Zhijian  Yuan,  and  Erkki  Oja.    Kullback-leibler
  divergence for nonnegative matrix factorization.  In <span 
class="ptmri8t-x-x-120">International Conference</span>
  <span 
class="ptmri8t-x-x-120">on Artificial Neural Networks</span>, pages 250&#8211;257. Springer, 2011.
  </p>
                                                                          

                                                                          
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlin2007convergence"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Chih-Jen  Lin.   On  the  convergence  of  multiplicative  update  algorithms  for
  nonnegative matrix factorization.  <span 
class="ptmri8t-x-x-120">IEEE Transactions on Neural Networks</span>, 18
  (6):1589&#8211;1596, 2007.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xguan2012nenmf"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Naiyang Guan, Dacheng Tao, Zhigang Luo, and Bo&#x00A0;Yuan. Nenmf: An optimal
  gradient method for nonnegative matrix factorization.   <span 
class="ptmri8t-x-x-120">IEEE Transactions on</span>
  <span 
class="ptmri8t-x-x-120">Signal Processing</span>, 60(6):2882&#8211;2898, 2012.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkim2008nonnegative"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Hyunsoo Kim and Haesun Park.  Nonnegative matrix factorization based on
  alternating nonnegativity constrained least squares and active set method. <span 
class="ptmri8t-x-x-120">SIAM</span>
  <span 
class="ptmri8t-x-x-120">journal on matrix analysis and applications</span>, 30(2):713&#8211;730, 2008.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlam2008non"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Edmund&#x00A0;Y Lam. Non-negative matrix factorization for images with laplacian
  noise.    In  <span 
class="ptmri8t-x-x-120">Circuits  and  Systems,  2008.  APCCAS  2008.  IEEE  Asia  Pacific</span>
  <span 
class="ptmri8t-x-x-120">Conference on</span>, pages 798&#8211;801. IEEE, 2008.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkong2011robust"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Deguang  Kong,  Chris  Ding,  and  Heng  Huang.   Robust  nonnegative  matrix
  factorization using l21-norm.   In <span 
class="ptmri8t-x-x-120">Proceedings of the 20th ACM international</span>
  <span 
class="ptmri8t-x-x-120">conference on Information and knowledge management</span>, pages 673&#8211;682. ACM,
  2011.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbarbu2013variational"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tudor  Barbu.   Variational  image  denoising  approach  with  diffusion  porous
  media flow. In <span 
class="ptmri8t-x-x-120">Abstract and Applied Analysis</span>, volume 2013. Hindawi, 2013.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xliu2015performance"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tongliang
  Liu and Dacheng Tao.  On the performance of manhattan nonnegative matrix
  factorization.  <span 
class="ptmri8t-x-x-120">IEEE Transactions on Neural Networks and Learning Systems</span>,
  27(9):1851&#8211;1863, September 2016. doi:<a 
href="http://dx.doi.org/10.1109/TNNLS.2015.2458986" >10.1109/TNNLS.2015.2458986</a>.
  </p>
                                                                          

                                                                          
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xscaless"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Cedric  Fevotte  and  Jerome  Idier.      Algorithms  for  nonnegative  matrix
  factorization with the <span 
class="eurm-10x-x-120">&#x03B2;</span>-divergence.   <span 
class="ptmri8t-x-x-120">Neural Computation</span>, 23(9):2421&#8211;2456,
  2011. doi:<a 
href="http://dx.doi.org/10.1162/NECO_a_00168" >10.1162/NECO_a_00168</a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="XWalck:1996cca"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Christian Walck.  <span 
class="ptmri8t-x-x-120">Hand-book on statistical distributions for experimentalists</span>.
  1996. URL <a 
href="http://www.fysik.su.se/~walck/suf9601.pdf" class="url" ><span 
class="t1-zi4r-0-x-x-120">http://www.fysik.su.se/~walck/suf9601.pdf</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsampat2005computer"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mehul&#x00A0;P  Sampat,  Mia&#x00A0;K  Markey,  Alan&#x00A0;C  Bovik,  et&#x00A0;al.    Computer-aided
  detection  and  diagnosis  in  mammography.    <span 
class="ptmri8t-x-x-120">Handbook  of  image  and  video</span>
  <span 
class="ptmri8t-x-x-120">processing</span>, 2(1):1195&#8211;1217, 2005.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbelhumeur1997eigenfaces"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Peter&#x00A0;N Belhumeur, João&#x00A0;P Hespanha, and David&#x00A0;J Kriegman.  Eigenfaces
  vs. fisherfaces: Recognition using class specific linear projection.   Technical
  report, Yale University New Haven United States, 1997.
</p>
  </div>
    
</body></html> 

                                                                          


